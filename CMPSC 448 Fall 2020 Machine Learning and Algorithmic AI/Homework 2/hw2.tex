\documentclass[11pt]{article}
\usepackage[top=2cm,bottom=2.5cm,left=2cm,right=2cm,marginparwidth=2cm]{geometry}
%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{listings}
\usepackage{pythonhighlight}
\usepackage{minted}

%% Sets page size and margins

\usepackage{float}
%% Useful packages
% \usepackage{amsmath}
\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{listings}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{ {./images/} }
% \DeclareGraphicsExtensions{.pdf,.jpg,.png}

\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{problem}{Problem}
\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\end{proof}}

%% defined colors
\definecolor{Blue}{rgb}{0,0,0.5}
\definecolor{Green}{rgb}{0,0.75,0.0}
\definecolor{LightGray}{rgb}{0.6,0.6,0.6}
\definecolor{DarkGray}{rgb}{0.3,0.3,0.3}

\title{CMPSC 448: Machine Learning and AI \\ Homework 2}
\author{}
\date{}

\begin{document}
\maketitle

\section*{Instruction}

This HW includes both theory and implementation problems:
\begin{itemize}
    \item You cannot look at anyone else's code
    \item Your homework must work with Python 3.7 (you may install the Anaconda distribution of Python)
    \item You need to submit a report including all deliverable and figures (in PDF format), also three files \textsf{Problem4.ipynb}, \textsf{Problem5.py}, and \textsf{Problem5Plot.py}
    \item The only modules your code can import are: \textsf{math}, \textsf{numpy}, \textsf{matplotlib}, \textsf{random}, \textsf{pandas}, \textsf{sklearn}
\end{itemize}


\section*{Theory and problem solving}
\begin{problem}[1]
[20 points] In the lectures, we showed that the MLE estimator for linear regression when the random noise for each data point is identically and independently distributed (i.i.d.) from a Gaussian distribution $\mathcal{N}(0, \sigma^2)$ reduces to the linear regression with squared loss (OLS). In this problem, we would like to change the distribution of the noise model and derive the MLE optimization problem. In particular, let’s assume for a fixed unknown linear model $\bm{w}_{\ast} \in \mathbb{R}^d$ the response $y_i$ for each data point $\bm{x}_{i} \in \mathbb{R}^d$ in training data $\mathcal{S} = \{(\bm{x}_{1}, y_1),(\bm{x}_{2}, y_2),\dots,(\bm{x}_{n}, y_n)\}$ is generated by
\begin{equation*}
  y_i = \bm{w}_{\ast}^{\top}\bm{x}_i + \epsilon_i
\end{equation*}
where $\epsilon_i$ is generated i.i.d. from a Laplace distribution $\epsilon_i \sim \textsf{Laplace}(0, \sigma)$.
A random variable $x$ has a $\textsf{Laplace}(\mu,\sigma)$ distribution if its probability density function is
\begin{equation*}
    f(x|\mu,\sigma) = \frac{1}{2\sigma}\exp(-\frac{|x-\mu|}{\sigma})
\end{equation*}

Under above assumption on the noise model,
\begin{enumerate}
    \item Show that each $y_i, i = 1, 2, . . . , n$ is a random variable that follows $\textsf{Laplace}(\bm{w}_{\ast}^{\top}\bm{x}_i, \sigma)$ distribution.
    \item Write down the MLE estimator for training data in $\mathcal{S}$ and derive the final optimization problem. Note that you just need to state the final minimization problem and not its solution.
    \item Compare the obtained optimization problem to one obtained under Gaussian noise model and highlight key differences.
\end{enumerate}
\end{problem}

\begin{problem}[2]
[15 points] Suppose we run a ridge regression with regularization parameter $\lambda$ on a training data with a single variable $\mathcal{S} = \{(x_{1}, y_1),(x_{2}, y_2),\dots,(x_{n}, y_n)\}$, and get coefficient $w_1 \in \mathbb{R}$ (for simplicity, we assumed the data are centered and no bias (intercept) term is needed).
We now include an exact copy of first feature to get a new training data as
\begin{equation*}
    \mathcal{S}^{\prime} = \{([x_{1}, x_{1}]^{\top}, y_1),([x_{2}, x_{2}]^{\top}, y_2),\dots,([x_{n}, x_{n}]^{\top}, y_n)\}   
\end{equation*}
where each training example is a 2 dimensional vector with equal coordinates, refit our ridge regression on $\mathcal{S}^{\prime}$ and get the solution $[w_1^{\prime}, w_2^{\prime}]^{\top} \in \mathbb{R}^{2}$.
\begin{enumerate}
    \item Derive the optimal solution for $[w_1^{\prime}, w_2^{\prime}]^{\top}$ and show that $w_1^{\prime} = w_2^{\prime}$.
    \item What is relation between $w_1^{\prime}$ and $w_1$.
\end{enumerate}
\end{problem}

\begin{problem}[3]
[10 points] As we discussed in the lecture, the Perceptron algorithm will converge only if the data is linearly separable. In particular, for linearly separable data with margin $\gamma$, if $\lVert\bm{x}\rVert_2 \leq R$ for all data points ,then it will converge in at most $\left(\frac{R}{\gamma}\right)^2$ iterations as stated in the class.

If the training data $\mathcal{S} = \{(\bm{x}_{1}, y_1),(\bm{x}_{2}, y_2),\dots,(\bm{x}_{n}, y_n)\}$ where $\bm{x}_{i} \in \mathbb{R}^{d} $ and $y_i \in \{-1, +1\}$ is not linearly separable, then there is a simple trick to force the data to be linearly separable and then apply the Perceptron algorithm as follows. If you have $n$ data points in $d$ dimensions, map data point $\bm{x}_{i}$ to the $(d+n)$-dimensional point $[\bm{x}_{i}, \bm{e}_{i}]^{\top} \in \mathbb{R}^{d+n}$, where $\bm{e}_{i} \in \{0,1\}^n$ is a $n$-dimensional vector of zeros, except for the $i$th position, which is 1 (e.g., $\bm{e}_{4}=[0,0,0,1,0,\dots,0]^{\top}$).

Show that if you apply this mapping, the data becomes linearly separable (you may wish to do so by providing a weight vector $\bm{w}$ in $(d+n)$-dimensional space that successfully separates the data).
\end{problem}

\section*{Programming and experiment}
\begin{problem}[4]
[25 points] In this problem you will use the Pima Indians Diabetes dataset from the UCI repository to experiment with the $k$-NN algorithm and find the optimal value for the number of neighbors $k$.
You do not need to implement the algorithm and encouraged to use the implementation in \textsf{scikit-learn}.
Below is a simple code showing the steps to use the NN implementation when the number of neighbors is 3:

\begin{minted}{python}
from sklearn.neighbors import KNeighborsClassifier
# Create NN classifier
knn = KNeighborsClassifier(n_neighbors = 3)
# Fit the classifier to the data
knn.fit(X_train,y_train)
# Predict the labels of test data
yhat = knn.predict(X_test)
# Or, directly check accuracy of our model on the test data
knn.score(X_test, y_test)
\end{minted}

To accomplish this task, please do:
\begin{itemize}
  \item Download the provided \textsf{Pima.csv} data file and load it using \textsf{pandas}. As a sanity check, make sure there are 768 rows of data (potential diabetes patients) and 9 columns (8 input features including Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, and 1 target output). Note that the data file has no header and you might want to explicitly create the header. The last value in each row contains the target label for that row, and the remaining values are the features. Report the statistics of each feature (min, max, average, standard deviation) and the histogram of the labels (target outputs).
  \item Split the data into training and test sets with 80\% training and 20\% test data sizes. You can easily do this in \textsf{scikit-learn}, e.g.,
  \begin{minted}{python}
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
  \end{minted}
  Use 5-fold cross-validation on training data to decide the best number of neighbours $k$. To this end, you can use the built in functionality in \textsf{scikit-learn} such as \textsf{cross\_val\_score} (note that this function returns the accuracy for each fold in an array and you have to average them to get the accuracy for all splits). For $k = 1, 2, 3, \dots, 15$ compute the 5-fold cross validation error and plot the results (with values of $k$ on the $x$-axis and accuracy on the $y$-axis). Include the plot in your report and justify your decision for picking a particular number of neighbors $k$.
  \item Evaluate the $k$-NN algorithm on test data with the optimal number of neighbours you obtained in previous step and report the test error.
  \item Process the input data by subtracting the mean (a.k.a. centralization) and dividing by the standard deviation (a.k.a. standardization) over each dimension (feature), repeat the previous part and report the accuracy. Do centralization and standardization affect the accuracy? Why?
\end{itemize}

\subsection*{Running and Deliverable}
You are provided with a \textsf{Problem4.ipynb} file to put the implementation code for all parts above. Make sure your notebook is runnable on Anaconda with Python 3.7. The results and discussions should be included in the PDF file.
\end{problem}

\begin{problem}[5]
[30 points] In this problem, we consider a simple linear regression model with a modified loss function and try to solve it with Gradient Descant (GD) and Stochastic Gradient Descant (SGD).

In general setting, the data has the form $\{(\bm{x}_{1}, y_1),(\bm{x}_{2}, y_2),\dots,(\bm{x}_{n}, y_n)\}$ where $\bm{x}_i$ is the $d$-dimensional feature vector and $y_i$ is a real-valued target. For this regression problem, we will be using linear prediction $\bm{w}^{\top}\bm{x}_i$ with the objective function:
$$
f(\bm{w}) = \frac{1}{n}\sum_{i=1}^{n}g_{\delta}(\bm{w};\bm{x}_{i}, y_i) + \lambda\sum_{j=1}^{d}w_j^2
$$
where $g_{\delta}(\bm{w};\bm{x}, y)$ is the error of linear model with parameter vector $\bm{w} \in \mathbb{R}^d$ on a single training pair $(\bm{x}, y)$ defined by:
$$
g_{\delta}(\bm{w};\bm{x}, y)=  \begin{cases}
     (y - \bm{w}^{\top}\bm{x} - \delta)^2  & \text{if } y \geq \bm{w}^{\top}\bm{x} + \delta\\
     0                                    & \text{if } \lvert y - \bm{w}^{\top}\bm{x} \rvert < \delta\\
     (y - \bm{w}^{\top}\bm{x} + \delta)^2  & \text{if } y \leq \bm{w}^{\top}\bm{x} - \delta
    \end{cases}
$$
Please note that we simply dropped the intercept term by the simple trick we discussed in the lecture, i,e., adding a constant feature, which is always equal to one to simplify estimation of the ``intercept" term.

\subsection*{Gradient Descent}
In this part, you are asked to optimize the above objective function using gradient descent and plot the function values over different iterations, which can be done using the python library \textsf{matplotlib}.

To this end, in \textsf{Problem5.py}, fill in the function \textsf{bgd\_l2(data, y, w, eta, delta, lam, num\_iter)} where \textsf{data} is a two dimensional numpy array with each row being a feature vector, \textsf{y} is a one-dimensional numpy array of target values, \textsf{w} is a one-dimensional numpy array corresponding to the weight vector, \textsf{eta} is the learning rate, \textsf{delta} and \textsf{lam} are parameters of the objective function. This function should return new weight vector, history of the value of objective function after each iteration (python list).

Run this function for the following settings and plot the history of objective function (you should expect a monotonically decreasing function over iteration number):

\begin{itemize}
    \item $\eta = 0.05, \delta = 0.1, \lambda = 0.001, \textsf{num\_iter}=50$
    \item $\eta = 0.1, \delta = 0.01, \lambda = 0.001, \textsf{num\_iter}=50$
    \item $\eta = 0.1, \delta = 0, \lambda = 0.001, \textsf{num\_iter}=100$
    \item $\eta = 0.1, \delta = 0, \lambda = 0, \textsf{num\_iter}=100$
\end{itemize}

\subsection*{Stochastic Gradient Descent}
In \textsf{Problem5.py} fill in the function \textsf{sgd\_l2(data, y, w, eta, delta, lam, num\_iter, i)} where \textsf{data, y, w, lam, delta, num\_iter} are same as previous part. In this part, you should use $\frac{n}{\sqrt{t}}$ as a learning rate, where \textsf{t} is the iteration number, starting from 1. The variable \textsf{i} is for testing the correctness of your function. If \textsf{i} set to −1 then you just need to apply the normal SGD (randomly select the data point), which runs for \textsf{num\_iter}, but if \textsf{i} set to something else (other than −1), your code only needs to compute \textbf{SGD} for that specific data point (in this case, the \textsf{num\_iter} will be 1!).

Run this function for the settings below and plot the history of objective function:

\begin{itemize}
    \item $\eta = 1, \delta = 0.1, \lambda = 0.5, \textsf{num\_iter}=800$
    \item $\eta = 1, \delta = 0.01, \lambda = 0.1, \textsf{num\_iter}=800$
    \item $\eta = 1, \delta = 0, \lambda = 0, \textsf{num\_iter}=40$
    \item $\eta = 1, \delta = 0, \lambda = 0, \textsf{num\_iter}=800$
\end{itemize}

\subsection*{Running and Deliverable}
You are provided with a sample data sets (\textsf{data.npy}). You can load these files into \textsf{numpy} array using this syntax: \textsf{np.load(`data.npy')}. The sample data for this homework has 100 data points $(x_i, y_i)$ in a $100 \times 2$ \textsf{numpy} array. Please note that you need to add a column of all ones to data to handle the intercept term, making your data a $100 \times 3$ \textsf{numpy} array. For the plotting part, make sure that your plots have appropriate title, $x$ and $y$-axis labels. You need to submit two python files \textsf{Problem5.py} and \textsf{Problem5Plot.py} and a PDF file including all the plots. In \textsf{Problem5.py} everything except the imports should be inside the functions definition we mentioned above. The file \textsf{Problem5Plot.py} is where you are generating the plots.
\end{problem}
\end{document}